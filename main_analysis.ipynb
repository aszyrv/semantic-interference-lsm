{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "667797b9-ef9b-44a7-b2b1-605b569532af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import os\n",
    "\n",
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from scipy.stats import pearsonr, ttest_ind, f_oneway\n",
    "from scipy.io import wavfile\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from navec import Navec\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn3\n",
    "%matplotlib qt\n",
    "\n",
    "font = {'font.family' : 'Arial',\n",
    "        'font.size'   : 7,\n",
    "        'pdf.fonttype': 42}\n",
    "\n",
    "plt.rcParams.update(font)\n",
    "# single col: 3.386\n",
    "# double col: 7.087\n",
    "# lines 0.25-1 pt\n",
    "# font at least 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a154107-a27f-4470-842c-b6da175520a2",
   "metadata": {},
   "source": [
    "### Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fc3ca5f-f649-40bf-80c8-8299b9f5da7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demographic and clinical features\n",
    "demo = pd.read_csv(op.join('data', 'demo.csv'))\n",
    "\n",
    "# behavioral data\n",
    "pwi_wide = pd.read_csv(op.join('data', 'PWI_data_25pat.csv'))\n",
    "pwi_lemma_freq = pd.read_csv(op.join('data', 'PWI_lemma_frequencies.csv'))\n",
    "pwi_lemmas = pd.read_csv(op.join('data', 'PWI_lemmatization.csv'))#.drop_duplicates(subset = 'Response')\n",
    "\n",
    "# neural data\n",
    "cortical_loads = pd.read_csv(op.join('data', 'harvoxf_batch_descriptives_25pat.csv'))\n",
    "tract_loads = pd.read_csv(op.join('data', 'tractotron_output', 'proportion.csv'))\n",
    "tract_probs = pd.read_csv(op.join('data', 'tractotron_output', 'probability.csv'))\n",
    "\n",
    "# reformatting PWI data to long format\n",
    "pwi_data = pd.DataFrame({})\n",
    "\n",
    "common_cols = ['Critical.segment', 'Target', 'Distractor', 'Condition']\n",
    "\n",
    "for sub in demo['ID'].unique():\n",
    "    \n",
    "    # subsetting current participant's data\n",
    "    subset = pwi_wide.loc[:,pwi_wide.columns.str.startswith(sub)]\n",
    "    subset = subset.rename(lambda c: c.replace(sub, '').strip('.'), axis=1)\n",
    "    \n",
    "    # adding common columns\n",
    "    subset_full = pd.concat([pwi_wide.loc[:,common_cols], subset], axis=1)\n",
    "    subset_full['ID'] = sub\n",
    "    \n",
    "    # concatenating to the full dataframe\n",
    "    pwi_data = pd.concat([pwi_data, subset_full])\n",
    "\n",
    "pwi_data['ResponseType'] = pwi_data['ResponseType'].astype(str)\n",
    "\n",
    "# subsetting neural predictors\n",
    "cortical_rois = {'Inferior Frontal Gyrus, pars opercularis': 'POper',\n",
    "                 'Inferior Frontal Gyrus, pars triangularis': 'PTri',\n",
    "                 'Precentral Gyrus': 'PreCG'}\n",
    "\n",
    "wm_rois = {'Superior_Londgitudinal_Fasciculus_I_Left': 'SLF_I',\n",
    "           'Superior_Londgitudinal_Fasciculus_II_Left': 'SLF_II',\n",
    "           'Superior_Londgitudinal_Fasciculus_III_Left': 'SLF_III'}\n",
    "\n",
    "# subsetting cortical rois\n",
    "neural_predictors = cortical_loads.loc[cortical_loads['roi_name'].isin(list(cortical_rois.keys())),\n",
    "                                       ['ID', 'roi_name', 'numVox', 'numVoxNotZero']]\n",
    "neural_predictors['load'] = neural_predictors['numVoxNotZero'] / neural_predictors['numVox']\n",
    "\n",
    "# long to wide\n",
    "neural_predictors = neural_predictors.pivot(index='ID', columns='roi_name', values='load')\n",
    "neural_predictors = neural_predictors.rename(cortical_rois, axis=1)\n",
    "\n",
    "# nans to zeros\n",
    "neural_predictors = neural_predictors.fillna(0)\n",
    "\n",
    "# subcortical rois\n",
    "tract_loads['ID'] = tract_loads['filename'].apply(lambda x: 'cprin' + x.split('_')[0])\n",
    "\n",
    "tract_loads_subset = tract_loads.loc[:,['ID'] + list(wm_rois.keys())]\n",
    "tract_loads_subset = tract_loads_subset.rename(wm_rois, axis=1).set_index('ID')\n",
    "\n",
    "neural_predictors = pd.concat([neural_predictors, tract_loads_subset], axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635bebf2-da42-4fdd-a7c7-b79df00135a4",
   "metadata": {},
   "source": [
    "### Fetching stimulus and response features\n",
    "\n",
    "* target-distractor dissimilarity\n",
    "* sequential response similarity\n",
    "* response frequency\n",
    "\n",
    "All similarities are Pearson correlations between embeddings extracted from a pre-trained GloVe model for Russian (https://github.com/natasha/navec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "950b56ec-02d2-43af-8544-70b70230a7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking nan values in lemmatization:\n",
      "0                                                     NaN\n",
      "66                                                    NaN\n",
      "132                                         не записалось\n",
      "133                                            нет ответа\n",
      "198                                         не записалось\n",
      "264                                                   NaN\n",
      "330                                                   NaN\n",
      "396                                                   NaN\n",
      "406                               спичка, конечно нет... \n",
      "416                                                   NaN\n",
      "423                                            нет ответа\n",
      "426                         за, за... ч.. к.. ну, не знаю\n",
      "454                               за, за... ш... не надо \n",
      "458                ээ кр, нет... щас щас щас... ко... кок\n",
      "462                                                   NaN\n",
      "469                                            нет ответа\n",
      "528                                 ошибка в предъявлении\n",
      "594                                         не записалось\n",
      "649                                         не записалось\n",
      "650                                 фа.. на на я... лампа\n",
      "660                                         не записалось\n",
      "726                                         не записалось\n",
      "746                                            нет ответа\n",
      "792                                         не записалось\n",
      "858                                         не записалось\n",
      "865                                            нет ответа\n",
      "879                                            нет ответа\n",
      "884                                            нет ответа\n",
      "890                                            нет ответа\n",
      "913                                            нет ответа\n",
      "924                                         не записалось\n",
      "990                                         не записалось\n",
      "991                                                  нрзб\n",
      "997                              это... не диван... забыл\n",
      "999            мммм.... забыл как называется, щас вспомню\n",
      "1001                                           нет ответа\n",
      "1016                                           нет ответа\n",
      "1019                                     не вспомню никак\n",
      "1028    черт, не помню, не помню, все знакомое а вспом...\n",
      "1030                                           нет ответа\n",
      "1032                             этот самый... не знаю...\n",
      "1033    это топ... не топор а как же он... не помню......\n",
      "1034    не помню как это... все знакомо, но не помню к...\n",
      "1037                    как же он опять-то... опять забыл\n",
      "1043                                           нет ответа\n",
      "1053       железное кровать.... не кровать, а... не помню\n",
      "1056                                        не записалось\n",
      "1089                   нет ответа, после подсказки назвал\n",
      "1122                                                  NaN\n",
      "1188                                                  NaN\n",
      "1254                                                  NaN\n",
      "1260                                             кралелет\n",
      "1261                                     пото, пото милек\n",
      "1308                                    я забыла, что это\n",
      "1320                                                  NaN\n",
      "1325                        это ой, как оно... не вспомню\n",
      "1338                                             пернатые\n",
      "1352                                    кол... ой, забыла\n",
      "1354                                         не вспомнила\n",
      "1375                 как вот это.. с.. с огнем связано...\n",
      "1386                                                  NaN\n",
      "1495                                          нет ответа?\n",
      "1518              повтрное объяснение задания, нет ответа\n",
      "1584                                                  NaN\n",
      "1610                                                  NaN\n",
      "Behavioral dataframe shape: (1650, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Critical.segment</th>\n",
       "      <th>Target</th>\n",
       "      <th>Distractor</th>\n",
       "      <th>Condition</th>\n",
       "      <th>Response</th>\n",
       "      <th>ResponseType</th>\n",
       "      <th>RT</th>\n",
       "      <th>Comments</th>\n",
       "      <th>ID</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>prev_lemma</th>\n",
       "      <th>consec_dissimilarity</th>\n",
       "      <th>distr_dissimilarity</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>logfreq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>автобус</td>\n",
       "      <td>троллейбус</td>\n",
       "      <td>semantic</td>\n",
       "      <td>аптобус</td>\n",
       "      <td>1p</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>-</td>\n",
       "      <td>cprin229</td>\n",
       "      <td>автобус</td>\n",
       "      <td>мотоцикл</td>\n",
       "      <td>0.439470</td>\n",
       "      <td>0.305089</td>\n",
       "      <td>64.8</td>\n",
       "      <td>4.171306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>автобус</td>\n",
       "      <td>автобус</td>\n",
       "      <td>congr</td>\n",
       "      <td>аптопульс</td>\n",
       "      <td>1p</td>\n",
       "      <td>1768.0</td>\n",
       "      <td>-</td>\n",
       "      <td>cprin229</td>\n",
       "      <td>автобус</td>\n",
       "      <td>лампа</td>\n",
       "      <td>0.906612</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.8</td>\n",
       "      <td>4.171306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3001</td>\n",
       "      <td>автобус</td>\n",
       "      <td>нож</td>\n",
       "      <td>unrel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>не записалось</td>\n",
       "      <td>cprin229</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.900023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4001</td>\n",
       "      <td>акула</td>\n",
       "      <td>кит</td>\n",
       "      <td>semantic</td>\n",
       "      <td>акола</td>\n",
       "      <td>1p</td>\n",
       "      <td>1396.0</td>\n",
       "      <td>-</td>\n",
       "      <td>cprin229</td>\n",
       "      <td>акула</td>\n",
       "      <td>ведро</td>\n",
       "      <td>0.942681</td>\n",
       "      <td>0.618731</td>\n",
       "      <td>9.3</td>\n",
       "      <td>2.230014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5001</td>\n",
       "      <td>акула</td>\n",
       "      <td>акула</td>\n",
       "      <td>congr</td>\n",
       "      <td>акола</td>\n",
       "      <td>1p</td>\n",
       "      <td>1465.0</td>\n",
       "      <td>-</td>\n",
       "      <td>cprin229</td>\n",
       "      <td>акула</td>\n",
       "      <td>зажигалка</td>\n",
       "      <td>0.876260</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.3</td>\n",
       "      <td>2.230014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Critical.segment   Target  Distractor Condition   Response ResponseType  \\\n",
       "0              1001  автобус  троллейбус  semantic    аптобус           1p   \n",
       "1              2001  автобус     автобус     congr  аптопульс           1p   \n",
       "2              3001  автобус         нож     unrel        NaN           99   \n",
       "3              4001    акула         кит  semantic      акола           1p   \n",
       "4              5001    акула       акула     congr      акола           1p   \n",
       "\n",
       "       RT       Comments        ID    Lemma prev_lemma  consec_dissimilarity  \\\n",
       "0  1072.0              -  cprin229  автобус   мотоцикл              0.439470   \n",
       "1  1768.0              -  cprin229  автобус      лампа              0.906612   \n",
       "2     NaN  не записалось  cprin229      NaN       None                   NaN   \n",
       "3  1396.0              -  cprin229    акула      ведро              0.942681   \n",
       "4  1465.0              -  cprin229    акула  зажигалка              0.876260   \n",
       "\n",
       "   distr_dissimilarity  Frequency   logfreq  \n",
       "0             0.305089       64.8  4.171306  \n",
       "1                  NaN       64.8  4.171306  \n",
       "2             0.900023        NaN       NaN  \n",
       "3             0.618731        9.3  2.230014  \n",
       "4                  NaN        9.3  2.230014  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading navec model\n",
    "navec = Navec.load(op.join('utils', 'navec_hudlit_v1_12B_500K_300d_100q.tar'))\n",
    "\n",
    "# nan vec for words missing in the model\n",
    "nan_vec = np.zeros(navec['<unk>'].shape)\n",
    "nan_vec[:] = np.nan\n",
    "\n",
    "# parsing the order of stimulus presentations\n",
    "pwi_trials = pd.DataFrame({})\n",
    "for file in os.listdir(op.join('data', 'PWI_trial_lists')):\n",
    "    with open(op.join('data', 'PWI_trial_lists', file), 'r', encoding='ANSI') as f:\n",
    "        text = f.read().strip().split('\\n')\n",
    "        segm = [int(l.strip('=').split(' ')[0]) for l in text]\n",
    "        patient = 'cprin' + file.split('.')[0]\n",
    "        pwi_trials = pd.concat([pwi_trials, pd.DataFrame({'ID': [patient]*len(segm),\n",
    "                                                          'Critical.segment': segm})])\n",
    "\n",
    "# adding responses\n",
    "pwi_trials = pwi_trials.merge(pwi_data[['ID', 'Critical.segment', 'Response']],\n",
    "                              how = 'left', on = ['ID', 'Critical.segment'])\n",
    "# adding response lemmas\n",
    "pwi_trials = pwi_trials.merge(pwi_lemmas, how = 'left', on = ['Response'])\n",
    "\n",
    "# checking nan values in lemmas\n",
    "print('Checking nan values in lemmatization:')\n",
    "print(pwi_trials.loc[pwi_trials['Lemma'].isna(), 'Response'].to_string())\n",
    "\n",
    "# adding a column containing previous responses' lemmas\n",
    "shifted_lemmas = []\n",
    "for sub in pwi_trials['ID'].unique():\n",
    "    subset = pwi_trials[pwi_trials['ID'] == sub]\n",
    "    shifted_lemmas += list(subset['Lemma'].shift(1).values)\n",
    "pwi_trials['prev_lemma'] = shifted_lemmas\n",
    "\n",
    "# calculating similarity to previous lemma\n",
    "dissimilarity = []\n",
    "for ri, r in pwi_trials.iterrows():\n",
    "    if r['Lemma'] != r['prev_lemma']:\n",
    "        try:\n",
    "            dissimilarity.append(1 - pearsonr(navec[r['Lemma']],\n",
    "                                              navec[r['prev_lemma']]).statistic)\n",
    "        except:\n",
    "            dissimilarity.append(np.nan)\n",
    "    else:\n",
    "        dissimilarity.append(np.nan)\n",
    "pwi_trials['consec_dissimilarity'] = dissimilarity\n",
    "\n",
    "# calculating target-distractor similarity\n",
    "pwi_stim = pwi_data[['Critical.segment', 'Condition', 'Target', 'Distractor']].drop_duplicates()\n",
    "\n",
    "dissimilarity = []\n",
    "for ri, r in pwi_stim.iterrows():\n",
    "    if r['Target'] != r['Distractor']:\n",
    "        try:\n",
    "            dissimilarity.append(1 - pearsonr(navec[r['Target']],\n",
    "                                              navec[r['Distractor']]).statistic)\n",
    "        except:\n",
    "            dissimilarity.append(np.nan)\n",
    "    else:\n",
    "        dissimilarity.append(np.nan)\n",
    "        \n",
    "pwi_stim['distr_dissimilarity'] = dissimilarity\n",
    "\n",
    "# adding similarity values to the original df\n",
    "pwi_data = pwi_data.merge(pwi_trials[['ID', 'Critical.segment', 'Lemma',\n",
    "                                      'prev_lemma', 'consec_dissimilarity']],\n",
    "                          how = 'left', on = ['ID', 'Critical.segment'])\n",
    "\n",
    "pwi_data = pwi_data.merge(pwi_stim[['Critical.segment', 'distr_dissimilarity']],\n",
    "                          how = 'left', on = 'Critical.segment')\n",
    "\n",
    "# adding lemma frequency\n",
    "pwi_data = pwi_data.merge(pwi_lemma_freq, how = 'left', on = 'Lemma')\n",
    "pwi_data['logfreq'] = np.log(pwi_data['Frequency'])\n",
    "\n",
    "print('Behavioral dataframe shape:', pwi_data.shape)\n",
    "pwi_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f672f858-b6dc-4f92-b10f-ef43b956a69b",
   "metadata": {},
   "source": [
    "### Methods: Demographic and clinical summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a17ca99c-2313-4f6e-8573-80ea9dc1b70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo['Birth.date'] = pd.to_datetime(demo['Birth.date'], format = \"%d.%m.%Y\")\n",
    "demo['Onset.date'] = pd.to_datetime(demo['Onset.date'], format = \"%d.%m.%Y\")\n",
    "demo['MRI.date'] = pd.to_datetime(demo['MRI.date'], format = \"%d.%m.%Y\")\n",
    "demo['Behaviour.date'] = pd.to_datetime(demo['Behaviour.date'], format = \"%d.%m.%Y\")\n",
    "\n",
    "demo['age'] = demo.apply(lambda r: relativedelta(r['Behaviour.date'],\n",
    "                                                 r['Birth.date']).years, axis = 1)\n",
    "demo['months post stroke'] = demo.apply(lambda r: relativedelta(r['Behaviour.date'],\n",
    "                                                                r['Onset.date']), axis = 1)\n",
    "demo['months post stroke'] = demo['months post stroke'].apply(lambda x: x.years*12 + x.months)\n",
    "demo['weeks to MRI'] = (demo['Behaviour.date'] - demo['MRI.date']).apply(lambda x: x.days/7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7ccb6c-ba58-4323-9506-ee2ee59da1ee",
   "metadata": {},
   "source": [
    "Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca4ecca3-a50d-425d-bbc3-9c3d63249ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age descriptives\n",
      "count    25.000000\n",
      "mean     55.520000\n",
      "std      10.190355\n",
      "min      37.000000\n",
      "25%      47.000000\n",
      "50%      56.000000\n",
      "75%      66.000000\n",
      "max      70.000000\n",
      "Name: age, dtype: float64\n",
      "\n",
      "Time post stroke, months\n",
      "count    25.000000\n",
      "mean     21.280000\n",
      "std      24.247887\n",
      "min       1.000000\n",
      "25%       5.000000\n",
      "50%      12.000000\n",
      "75%      27.000000\n",
      "max      88.000000\n",
      "Name: months post stroke, dtype: float64\n",
      "\n",
      "Time between assessment and MRI, weeks\n",
      "count    25.000000\n",
      "mean     16.754286\n",
      "std      25.292221\n",
      "min       0.428571\n",
      "25%       1.142857\n",
      "50%       2.428571\n",
      "75%      26.000000\n",
      "max      86.285714\n",
      "Name: weeks to MRI, dtype: float64\n",
      "\n",
      "Time between assessment and MRI, weeks, for those with less than one year post stroke\n",
      "count    12.000000\n",
      "mean      2.059524\n",
      "std       1.211354\n",
      "min       0.428571\n",
      "25%       1.107143\n",
      "50%       1.785714\n",
      "75%       2.857143\n",
      "max       4.285714\n",
      "Name: weeks to MRI, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Age descriptives')\n",
    "print(demo['age'].describe())\n",
    "\n",
    "print('\\nTime post stroke, months')\n",
    "print(demo['months post stroke'].describe())\n",
    "\n",
    "print('\\nTime between assessment and MRI, weeks')\n",
    "print(np.abs(demo['weeks to MRI']).describe())\n",
    "\n",
    "print('\\nTime between assessment and MRI, weeks, for those with less than one year post stroke')\n",
    "print(np.abs(demo.loc[demo['months post stroke']<12, 'weeks to MRI']).describe())\n",
    "\n",
    "# saving demo for supplementay\n",
    "demo[['ID', 'age', 'Sex', 'Handedness', 'Etiology',\n",
    "      'Aphasia.type', 'Dysarhtria', 'months post stroke', 'weeks to MRI']].to_csv(op.join('output', 'suppl_1.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0d653e-c143-4e43-ba4f-26a10526ebbd",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Summary of performance accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fb9c85a-0d6d-4322-afe6-5fb2933c3461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all response types: ['1p' '99' '1' '31p' '31' '21p' '3' '0' '32p' '2' '21' '2p']\n",
      "all correct response types: ['1p' '1' '31p' '31' '21p' '21']\n",
      "total n correct responses: 1476\n",
      "total n techical errors: 39\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">sum</th>\n",
       "      <th colspan=\"2\" halign=\"left\">mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>correct</th>\n",
       "      <th>incorrect</th>\n",
       "      <th>correct</th>\n",
       "      <th>incorrect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>59.040000</td>\n",
       "      <td>5.400000</td>\n",
       "      <td>0.916366</td>\n",
       "      <td>0.083634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.630867</td>\n",
       "      <td>5.686241</td>\n",
       "      <td>0.087597</td>\n",
       "      <td>0.087597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>45.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>57.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>0.016129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>61.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.046875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>63.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.983871</td>\n",
       "      <td>0.109375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>65.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             sum                  mean           \n",
       "         correct  incorrect    correct  incorrect\n",
       "count  25.000000  25.000000  25.000000  25.000000\n",
       "mean   59.040000   5.400000   0.916366   0.083634\n",
       "std     5.630867   5.686241   0.087597   0.087597\n",
       "min    45.000000   0.000000   0.692308   0.000000\n",
       "25%    57.000000   1.000000   0.890625   0.016129\n",
       "50%    61.000000   3.000000   0.953125   0.046875\n",
       "75%    63.000000   7.000000   0.983871   0.109375\n",
       "max    65.000000  20.000000   1.000000   0.307692"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('all response types:', pwi_data['ResponseType'].unique())\n",
    "\n",
    "pwi_data['correct'] = pwi_data['ResponseType'].str.endswith('1') + pwi_data['ResponseType'].str.endswith('1p')\n",
    "pwi_data['incorrect'] = pwi_data['ResponseType'].isin(['3', '0', '32p', '2', '2p'])\n",
    "\n",
    "print('all correct response types:', pwi_data.loc[pwi_data['correct'], 'ResponseType'].unique())\n",
    "print('total n correct responses:', pwi_data['correct'].sum())\n",
    "print('total n techical errors:', (pwi_data['ResponseType'] == '99').sum())\n",
    "\n",
    "pwi_accuracy_summary = pwi_data[pwi_data['ResponseType'] != '99'].pivot_table(index='ID',\n",
    "                                                                              values = ['correct', 'incorrect'],\n",
    "                                                                              aggfunc = ['sum', 'mean'])\n",
    "pwi_accuracy_summary.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23e9d7cd-876d-4caf-9aec-01cc8487ba53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n incorrect responses: 135\n",
      "% incorrect responses: 8.181818181818182\n"
     ]
    }
   ],
   "source": [
    "print('n incorrect responses:', pwi_data['incorrect'].sum())\n",
    "print('% incorrect responses:', pwi_data['incorrect'].sum()*100/pwi_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95248b1b-c21e-49d9-97fd-894675971e5f",
   "metadata": {},
   "source": [
    "Cleaning data, summarizing the number of excluded trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "483652ae-8bba-45fa-a709-64bcbc5e0929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excluding incorrect trials: 220 13.3\n",
      "excluding trials with missing RTs: 160 9.7\n",
      "excluding trials with outlier logRTs: 14 0.8\n",
      "clean dataframe shape: (1256, 20) \n",
      "percentage of trials for modeling: 76.12121212121212\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Critical.segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>50.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.713736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>44.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>52.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>59.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>62.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Critical.segment\n",
       "count         25.000000\n",
       "mean          50.240000\n",
       "std            9.713736\n",
       "min           28.000000\n",
       "25%           44.000000\n",
       "50%           52.000000\n",
       "75%           59.000000\n",
       "max           62.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_n_trials = pwi_data.shape[0]\n",
    "pwi_data_clean = pwi_data.copy()\n",
    "\n",
    "# excluding incorrect responses\n",
    "correct_filter = pwi_data_clean['ResponseType'].isin(['1', '1p'])\n",
    "pwi_data_clean = pwi_data_clean.loc[correct_filter]\n",
    "print('excluding incorrect trials:',\n",
    "      sum(~correct_filter),\n",
    "      round(sum(~correct_filter)*100/orig_n_trials, 1))\n",
    "\n",
    "# excluding trials with missing RT\n",
    "RT_filter = ~pwi_data_clean['RT'].isna()\n",
    "pwi_data_clean = pwi_data_clean.loc[RT_filter]\n",
    "print('excluding trials with missing RTs:',\n",
    "      sum(~RT_filter),\n",
    "      round(sum(~RT_filter)*100/orig_n_trials, 1))\n",
    "\n",
    "# excluding trials with logRT > +- 3SDs\n",
    "# centering logRT to individual participants' means\n",
    "pwi_data_clean['logRT'] = np.log(pwi_data_clean['RT'])\n",
    "RT_thresh = pwi_data_clean.pivot_table(index = ['ID', 'Condition'],\n",
    "                                       values = 'logRT',\n",
    "                                       aggfunc = ['mean', 'std']).reset_index()\n",
    "RT_thresh['RT_thresh_lower'] = RT_thresh[('mean', 'logRT')] - 3*RT_thresh[('std', 'logRT')]\n",
    "RT_thresh['RT_thresh_upper'] = RT_thresh[('mean', 'logRT')] + 3*RT_thresh[('std', 'logRT')]\n",
    "\n",
    "pwi_data_clean = pwi_data_clean.merge(RT_thresh[['ID', 'Condition', 'RT_thresh_lower', 'RT_thresh_upper']].droplevel(1, axis=1),\n",
    "                                      how = 'left',\n",
    "                                      on = ['ID', 'Condition'])\n",
    "\n",
    "RT_outlier_filter = (pwi_data_clean['logRT'] > pwi_data_clean['RT_thresh_lower']) * \\\n",
    "                    (pwi_data_clean['logRT'] < pwi_data_clean['RT_thresh_upper'])\n",
    "\n",
    "pwi_data_clean = pwi_data_clean.loc[RT_outlier_filter]\n",
    "\n",
    "print('excluding trials with outlier logRTs:',\n",
    "      sum(~RT_outlier_filter),\n",
    "      round(sum(~RT_outlier_filter)*100/orig_n_trials, 1))\n",
    "\n",
    "print('clean dataframe shape:', pwi_data_clean.shape, '\\npercentage of trials for modeling:', pwi_data_clean.shape[0]*100/pwi_data.shape[0])\n",
    "\n",
    "pwi_nincl_summary = pwi_data_clean.pivot_table(index='ID',\n",
    "                                               values = 'Critical.segment',\n",
    "                                               aggfunc = 'count')\n",
    "pwi_nincl_summary.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc6bf41b-cb12-4fb5-a6d8-c4509d0bc764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape after adding neural predictors: (1256, 26)\n"
     ]
    }
   ],
   "source": [
    "# adding neural predictors for model fitting\n",
    "pwi_data_clean_withpreds = pwi_data_clean.copy()\n",
    "pwi_data_clean_withpreds = pwi_data_clean_withpreds.merge(neural_predictors, how = 'left', on = 'ID')\n",
    "print('Dataframe shape after adding neural predictors:', pwi_data_clean_withpreds.shape)\n",
    "pwi_data_clean_withpreds.to_csv('pwi_data_clean_withpreds.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a56b4ec-8080-471e-a72d-4983f42f32be",
   "metadata": {},
   "source": [
    "T-test comparing target-distractor dissimilarity between conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bf72791-bef0-4997-a7d9-0973dc68c58a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TtestResult(statistic=-12.4240330273628, pvalue=2.5620986832115902e-15, df=40.03548502821155)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest_ind(pwi_stim.loc[pwi_stim['Condition'] == 'semantic', 'distr_dissimilarity'],\n",
    "          pwi_stim.loc[pwi_stim['Condition'] == 'unrel', 'distr_dissimilarity'],\n",
    "          axis=0, equal_var=False, nan_policy='raise', alternative='two-sided')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fa8273-2f68-4935-a02e-2f451aaeab70",
   "metadata": {},
   "source": [
    "ANOVA testing whether response dissimilarity differs between conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47c46f52-65bf-4c67-b056-d7adea7286fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "F_onewayResult(statistic=0.3831739661787659, pvalue=0.6817778823068653)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_oneway(pwi_data_clean.loc[pwi_data_clean['Condition'] == 'semantic', 'consec_dissimilarity'].dropna(), \n",
    "         pwi_data_clean.loc[pwi_data_clean['Condition'] == 'congr', 'consec_dissimilarity'].dropna(),\n",
    "         pwi_data_clean.loc[pwi_data_clean['Condition'] == 'unrel', 'consec_dissimilarity'].dropna(),\n",
    "         axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51493b5-a3fb-4c56-a854-dc2e46429d8f",
   "metadata": {},
   "source": [
    "Correlating target-distractor dissimilarity and consecutive response dissimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "708da03e-ab1a-4859-ae44-6eb8740ed55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PearsonRResult(statistic=-0.030409832099655164, pvalue=0.40375053364077834)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_corr_data = pwi_data_clean.dropna(subset=['consec_dissimilarity', 'distr_dissimilarity'])\n",
    "pearsonr(temp_corr_data['consec_dissimilarity'], temp_corr_data['distr_dissimilarity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39617184-4ad4-4d26-83f8-5b1fcd6c3996",
   "metadata": {},
   "source": [
    "Fitting mixed-effects models for behavioral effects (Supplementary Tables 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0997a226-a653-47f1-b2f9-a78f983efe87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"z-scoring continuous predictors:\"\n",
      "[1] \"Age\"                  \"Post.onset.weeks\"     \"consec_dissimilarity\"\n",
      "[4] \"distr_dissimilarity\"  \"logfreq\"             \n",
      "[1] \"gender coding:\"\n",
      "  [,1]\n",
      "f    1\n",
      "m   -1\n",
      "[1] \"distractor condition coding\"\n",
      "         congr unrel\n",
      "semantic     0     0\n",
      "congr        1     0\n",
      "unrel        0     1\n",
      "Linear mixed model fit by REML. t-tests use Satterthwaite's method [\n",
      "lmerModLmerTest]\n",
      "Formula: logRT ~ Condition * consec_dissimilarity + logfreq + Age + Sex +  \n",
      "    Post.onset.weeks + (1 | ID) + (1 | Critical.segment)\n",
      "   Data: pwi_data\n",
      "Control: lmerControl(optimizer = \"bobyqa\")\n",
      "\n",
      "REML criterion at convergence: 1187.2\n",
      "\n",
      "Scaled residuals: \n",
      "    Min      1Q  Median      3Q     Max \n",
      "-2.5325 -0.6212 -0.1370  0.4316  5.6654 \n",
      "\n",
      "Random effects:\n",
      " Groups           Name        Variance Std.Dev.\n",
      " Critical.segment (Intercept) 0.008094 0.08997 \n",
      " ID               (Intercept) 0.078206 0.27965 \n",
      " Residual                     0.136663 0.36968 \n",
      "Number of obs: 1200, groups:  Critical.segment, 66; ID, 25\n",
      "\n",
      "Fixed effects:\n",
      "                                      Estimate Std. Error         df t value\n",
      "(Intercept)                          7.318e+00  6.353e-02  2.934e+01 115.201\n",
      "Conditioncongr                      -2.307e-01  3.792e-02  6.096e+01  -6.085\n",
      "Conditionunrel                       6.057e-03  3.847e-02  6.408e+01   0.157\n",
      "consec_dissimilarity                -2.567e-04  2.240e-02  1.162e+03  -0.011\n",
      "logfreq                             -2.400e-02  1.495e-02  8.778e+01  -1.606\n",
      "Age                                 -2.324e-02  5.777e-02  2.096e+01  -0.402\n",
      "Sex1                                 2.193e-02  5.893e-02  2.107e+01   0.372\n",
      "Post.onset.weeks                     1.633e-02  5.803e-02  2.098e+01   0.281\n",
      "Conditioncongr:consec_dissimilarity -1.139e-02  2.867e-02  1.163e+03  -0.397\n",
      "Conditionunrel:consec_dissimilarity -9.789e-03  2.896e-02  1.164e+03  -0.338\n",
      "                                    Pr(>|t|)    \n",
      "(Intercept)                          < 2e-16 ***\n",
      "Conditioncongr                      8.44e-08 ***\n",
      "Conditionunrel                         0.875    \n",
      "consec_dissimilarity                   0.991    \n",
      "logfreq                                0.112    \n",
      "Age                                    0.692    \n",
      "Sex1                                   0.713    \n",
      "Post.onset.weeks                       0.781    \n",
      "Conditioncongr:consec_dissimilarity    0.691    \n",
      "Conditionunrel:consec_dissimilarity    0.735    \n",
      "---\n",
      "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
      "\n",
      "Correlation of Fixed Effects:\n",
      "            (Intr) Cndtnc Cndtnn cnsc_d logfrq Age    Sex1   Pst.n. Cndtnc:_\n",
      "Conditncngr -0.315                                                          \n",
      "Conditinnrl -0.310  0.520                                                   \n",
      "cnsc_dssmlr -0.008  0.015  0.017                                            \n",
      "logfreq      0.004 -0.016  0.008  0.078                                     \n",
      "Age         -0.023  0.002  0.002 -0.002  0.004                              \n",
      "Sex1         0.187  0.003  0.003  0.005 -0.003 -0.043                       \n",
      "Pst.nst.wks -0.024  0.001  0.002  0.001 -0.001 -0.005 -0.149                \n",
      "Cndtncngr:_  0.006 -0.019 -0.013 -0.783 -0.034  0.007 -0.003 -0.003         \n",
      "Cndtnnrl:c_  0.006 -0.012 -0.004 -0.767 -0.022  0.005 -0.003  0.005  0.601  \n",
      "Linear mixed model fit by REML. t-tests use Satterthwaite's method [\n",
      "lmerModLmerTest]\n",
      "Formula: logRT ~ distr_dissimilarity * consec_dissimilarity + logfreq +  \n",
      "    Age + Sex + Post.onset.weeks + (1 | ID) + (1 | Critical.segment)\n",
      "   Data: pwi_data %>% filter(Condition != \"congr\")\n",
      "Control: lmerControl(optimizer = \"bobyqa\")\n",
      "\n",
      "REML criterion at convergence: 872.7\n",
      "\n",
      "Scaled residuals: \n",
      "    Min      1Q  Median      3Q     Max \n",
      "-2.9177 -0.5868 -0.1589  0.4088  4.8631 \n",
      "\n",
      "Random effects:\n",
      " Groups           Name        Variance Std.Dev.\n",
      " Critical.segment (Intercept) 0.007881 0.08877 \n",
      " ID               (Intercept) 0.123054 0.35079 \n",
      " Residual                     0.154886 0.39355 \n",
      "Number of obs: 756, groups:  Critical.segment, 44; ID, 25\n",
      "\n",
      "Fixed effects:\n",
      "                                           Estimate Std. Error         df\n",
      "(Intercept)                                7.320925   0.074512  22.477103\n",
      "distr_dissimilarity                        0.004937   0.019981  40.005204\n",
      "consec_dissimilarity                      -0.010835   0.015595 729.149325\n",
      "logfreq                                   -0.020763   0.018803  68.473037\n",
      "Age                                       -0.024309   0.072631  20.937244\n",
      "Sex1                                       0.006204   0.074136  21.091552\n",
      "Post.onset.weeks                           0.006930   0.073008  21.010122\n",
      "distr_dissimilarity:consec_dissimilarity   0.001132   0.014926 726.718665\n",
      "                                         t value Pr(>|t|)    \n",
      "(Intercept)                               98.252   <2e-16 ***\n",
      "distr_dissimilarity                        0.247    0.806    \n",
      "consec_dissimilarity                      -0.695    0.487    \n",
      "logfreq                                   -1.104    0.273    \n",
      "Age                                       -0.335    0.741    \n",
      "Sex1                                       0.084    0.934    \n",
      "Post.onset.weeks                           0.095    0.925    \n",
      "distr_dissimilarity:consec_dissimilarity   0.076    0.940    \n",
      "---\n",
      "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
      "\n",
      "Correlation of Fixed Effects:\n",
      "            (Intr) dstr_d cnsc_d logfrq Age    Sex1   Pst.n.\n",
      "dstr_dssmlr  0.004                                          \n",
      "cnsc_dssmlr  0.000  0.043                                   \n",
      "logfreq      0.008  0.122  0.125                            \n",
      "Age         -0.024  0.004  0.003  0.005                     \n",
      "Sex1         0.201  0.005  0.007  0.000 -0.042              \n",
      "Pst.nst.wks -0.026  0.003  0.008  0.002 -0.006 -0.149       \n",
      "dstr_dssm:_  0.005 -0.011 -0.149 -0.028  0.009 -0.005  0.008\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cmd = 'C:\\\\\"Program Files\"\\\\R\\\\R-4.2.2\\\\bin\\\\Rscript utils\\\\behav_model_fitting.R'\n",
    "output = subprocess.check_output(cmd, universal_newlines=True, shell=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff736456-ca88-4773-a3fd-421f24f2bd62",
   "metadata": {},
   "source": [
    "Fitting mixed-effects models for lesion load effects (Supplementary Tables 4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "547db91c-14b8-4658-a68f-6a5e2c12c9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"z-scoring continuous predictors:\"\n",
      " [1] \"Age\"                  \"Post.onset.weeks\"     \"consec_dissimilarity\"\n",
      " [4] \"distr_dissimilarity\"  \"logfreq\"              \"POper\"               \n",
      " [7] \"PTri\"                 \"PreCG\"                \"SLF_I\"               \n",
      "[10] \"SLF_II\"              \n",
      "[1] \"gender coding:\"\n",
      "  [,1]\n",
      "f    1\n",
      "m   -1\n",
      "[1] \"condition coding:\"\n",
      "         congr unrel\n",
      "semantic     0     0\n",
      "congr        1     0\n",
      "unrel        0     1\n",
      "[1] \"variance inflation factors:\"\n",
      "                                 GVIF Df GVIF^(1/(2*Df))\n",
      "POper                        2.299115  1        1.516283\n",
      "Condition                    1.004116  2        1.001027\n",
      "consec_dissimilarity         1.022653  1        1.011263\n",
      "PTri                         2.708588  1        1.645779\n",
      "SLF_I                        4.586733  1        2.141666\n",
      "SLF_II                       4.512703  1        2.124312\n",
      "PreCG                        4.021994  1        2.005491\n",
      "logfreq                      1.018088  1        1.009004\n",
      "Age                          1.093330  1        1.045624\n",
      "Sex                          1.334856  1        1.155360\n",
      "Post.onset.weeks             1.345105  1        1.159787\n",
      "POper:Condition              3.868737  2        1.402466\n",
      "POper:consec_dissimilarity   1.921130  1        1.386048\n",
      "Condition:PTri               5.575103  2        1.536608\n",
      "consec_dissimilarity:PTri    2.290734  1        1.513517\n",
      "Condition:SLF_I             18.112080  2        2.062966\n",
      "consec_dissimilarity:SLF_I   4.236922  1        2.058379\n",
      "Condition:SLF_II            15.896996  2        1.996773\n",
      "consec_dissimilarity:SLF_II  4.300452  1        2.073753\n",
      "Condition:PreCG             12.644349  2        1.885707\n",
      "consec_dissimilarity:PreCG   3.808306  1        1.951488\n",
      "Linear mixed model fit by REML. t-tests use Satterthwaite's method [\n",
      "lmerModLmerTest]\n",
      "Formula: \n",
      "logRT ~ POper * (Condition + consec_dissimilarity) + PTri * (Condition +  \n",
      "    consec_dissimilarity) + SLF_I * (Condition + consec_dissimilarity) +  \n",
      "    SLF_II * (Condition + consec_dissimilarity) + PreCG * (Condition +  \n",
      "    consec_dissimilarity) + logfreq + Age + Sex + Post.onset.weeks +  \n",
      "    (1 | ID) + (1 | Critical.segment)\n",
      "   Data: pwi_data\n",
      "Control: lmerControl(optimizer = \"bobyqa\")\n",
      "\n",
      "REML criterion at convergence: 1210.2\n",
      "\n",
      "Scaled residuals: \n",
      "    Min      1Q  Median      3Q     Max \n",
      "-2.6677 -0.6040 -0.1432  0.4225  5.4638 \n",
      "\n",
      "Random effects:\n",
      " Groups           Name        Variance Std.Dev.\n",
      " Critical.segment (Intercept) 0.008776 0.09368 \n",
      " ID               (Intercept) 0.078477 0.28014 \n",
      " Residual                     0.130903 0.36181 \n",
      "Number of obs: 1200, groups:  Critical.segment, 66; ID, 25\n",
      "\n",
      "Fixed effects:\n",
      "                              Estimate Std. Error         df t value Pr(>|t|)\n",
      "(Intercept)                  7.316e+00  6.433e-02  2.248e+01 113.736  < 2e-16\n",
      "POper                       -1.601e-01  8.692e-02  1.827e+01  -1.842  0.08178\n",
      "Conditioncongr              -2.306e-01  3.839e-02  6.133e+01  -6.007 1.12e-07\n",
      "Conditionunrel               6.465e-03  3.889e-02  6.413e+01   0.166  0.86850\n",
      "consec_dissimilarity        -1.363e-02  1.104e-02  1.157e+03  -1.234  0.21729\n",
      "PTri                         1.877e-01  9.497e-02  1.827e+01   1.977  0.06335\n",
      "SLF_I                       -2.064e-01  1.294e-01  1.791e+01  -1.595  0.12813\n",
      "SLF_II                       1.355e-01  1.189e-01  1.839e+01   1.140  0.26900\n",
      "PreCG                       -1.983e-02  1.184e-01  1.807e+01  -0.168  0.86883\n",
      "logfreq                     -2.311e-02  1.508e-02  8.971e+01  -1.532  0.12900\n",
      "Age                         -5.628e-03  6.040e-02  1.597e+01  -0.093  0.92691\n",
      "Sex1                         3.337e-02  6.731e-02  1.601e+01   0.496  0.62676\n",
      "Post.onset.weeks             2.187e-02  6.660e-02  1.602e+01   0.328  0.74688\n",
      "POper:Conditioncongr         8.717e-02  3.562e-02  1.107e+03   2.448  0.01454\n",
      "POper:Conditionunrel        -1.565e-02  3.667e-02  1.108e+03  -0.427  0.66959\n",
      "POper:consec_dissimilarity   3.432e-02  1.492e-02  1.130e+03   2.300  0.02163\n",
      "Conditioncongr:PTri         -1.957e-01  3.876e-02  1.109e+03  -5.050 5.17e-07\n",
      "Conditionunrel:PTri          1.015e-02  4.011e-02  1.112e+03   0.253  0.80036\n",
      "consec_dissimilarity:PTri   -3.023e-02  1.589e-02  1.132e+03  -1.902  0.05741\n",
      "Conditioncongr:SLF_I         1.453e-01  5.159e-02  1.105e+03   2.815  0.00496\n",
      "Conditionunrel:SLF_I        -3.012e-02  5.303e-02  1.106e+03  -0.568  0.57014\n",
      "consec_dissimilarity:SLF_I  -1.360e-02  2.143e-02  1.133e+03  -0.635  0.52574\n",
      "Conditioncongr:SLF_II       -1.930e-03  5.019e-02  1.110e+03  -0.038  0.96932\n",
      "Conditionunrel:SLF_II       -1.519e-03  5.202e-02  1.114e+03  -0.029  0.97671\n",
      "consec_dissimilarity:SLF_II  1.603e-02  2.394e-02  1.141e+03   0.670  0.50325\n",
      "Conditioncongr:PreCG        -3.320e-02  4.772e-02  1.104e+03  -0.696  0.48667\n",
      "Conditionunrel:PreCG         4.852e-02  4.884e-02  1.109e+03   0.993  0.32075\n",
      "consec_dissimilarity:PreCG   1.946e-02  2.220e-02  1.143e+03   0.877  0.38089\n",
      "                               \n",
      "(Intercept)                 ***\n",
      "POper                       .  \n",
      "Conditioncongr              ***\n",
      "Conditionunrel                 \n",
      "consec_dissimilarity           \n",
      "PTri                        .  \n",
      "SLF_I                          \n",
      "SLF_II                         \n",
      "PreCG                          \n",
      "logfreq                        \n",
      "Age                            \n",
      "Sex1                           \n",
      "Post.onset.weeks               \n",
      "POper:Conditioncongr        *  \n",
      "POper:Conditionunrel           \n",
      "POper:consec_dissimilarity  *  \n",
      "Conditioncongr:PTri         ***\n",
      "Conditionunrel:PTri            \n",
      "consec_dissimilarity:PTri   .  \n",
      "Conditioncongr:SLF_I        ** \n",
      "Conditionunrel:SLF_I           \n",
      "consec_dissimilarity:SLF_I     \n",
      "Conditioncongr:SLF_II          \n",
      "Conditionunrel:SLF_II          \n",
      "consec_dissimilarity:SLF_II    \n",
      "Conditioncongr:PreCG           \n",
      "Conditionunrel:PreCG           \n",
      "consec_dissimilarity:PreCG     \n",
      "---\n",
      "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
      "[1] \"fitting a model with continuous target-distractor dissimilarity\"\n",
      "[1] \"variance inflation factors:\"\n",
      "                      POper         distr_dissimilarity \n",
      "                   2.142371                    1.017394 \n",
      "       consec_dissimilarity                        PTri \n",
      "                   1.028371                    2.528776 \n",
      "                      SLF_I                      SLF_II \n",
      "                   4.324369                    4.207466 \n",
      "                      PreCG                         Age \n",
      "                   3.767754                    1.093130 \n",
      "                        Sex            Post.onset.weeks \n",
      "                   1.334244                    1.346650 \n",
      "                    logfreq   POper:distr_dissimilarity \n",
      "                   1.038783                    1.908389 \n",
      " POper:consec_dissimilarity    distr_dissimilarity:PTri \n",
      "                   1.779355                    2.328379 \n",
      "  consec_dissimilarity:PTri   distr_dissimilarity:SLF_I \n",
      "                   2.315018                    4.110743 \n",
      " consec_dissimilarity:SLF_I  distr_dissimilarity:SLF_II \n",
      "                   4.418745                    3.849527 \n",
      "consec_dissimilarity:SLF_II   distr_dissimilarity:PreCG \n",
      "                   4.363665                    3.402807 \n",
      " consec_dissimilarity:PreCG \n",
      "                   3.657575 \n",
      "Linear mixed model fit by REML. t-tests use Satterthwaite's method [\n",
      "lmerModLmerTest]\n",
      "Formula: logRT ~ POper * (distr_dissimilarity + consec_dissimilarity) +  \n",
      "    PTri * (distr_dissimilarity + consec_dissimilarity) + SLF_I *  \n",
      "    (distr_dissimilarity + consec_dissimilarity) + SLF_II * (distr_dissimilarity +  \n",
      "    consec_dissimilarity) + PreCG * (distr_dissimilarity + consec_dissimilarity) +  \n",
      "    Age + Sex + Post.onset.weeks + logfreq + (1 | ID) + (1 |  \n",
      "    Critical.segment)\n",
      "   Data: pwi_data %>% filter(Condition != \"congr\")\n",
      "Control: lmerControl(optimizer = \"bobyqa\")\n",
      "\n",
      "REML criterion at convergence: 909.3\n",
      "\n",
      "Scaled residuals: \n",
      "    Min      1Q  Median      3Q     Max \n",
      "-2.6685 -0.5730 -0.1233  0.4272  4.7781 \n",
      "\n",
      "Random effects:\n",
      " Groups           Name        Variance Std.Dev.\n",
      " Critical.segment (Intercept) 0.008466 0.09201 \n",
      " ID               (Intercept) 0.121853 0.34907 \n",
      " Residual                     0.151258 0.38892 \n",
      "Number of obs: 756, groups:  Critical.segment, 44; ID, 25\n",
      "\n",
      "Fixed effects:\n",
      "                              Estimate Std. Error         df t value Pr(>|t|)\n",
      "(Intercept)                  7.324e+00  7.503e-02  1.718e+01  97.604  < 2e-16\n",
      "POper                       -1.694e-01  1.050e-01  1.588e+01  -1.614  0.12627\n",
      "distr_dissimilarity          4.564e-03  2.022e-02  4.013e+01   0.226  0.82254\n",
      "consec_dissimilarity        -1.123e-02  1.535e-02  7.196e+02  -0.731  0.46488\n",
      "PTri                         1.975e-01  1.147e-01  1.592e+01   1.722  0.10448\n",
      "SLF_I                       -2.212e-01  1.570e-01  1.586e+01  -1.409  0.17808\n",
      "SLF_II                       1.376e-01  1.436e-01  1.599e+01   0.958  0.35214\n",
      "PreCG                        6.778e-03  1.432e-01  1.583e+01   0.047  0.96284\n",
      "Age                          1.751e-03  7.547e-02  1.592e+01   0.023  0.98178\n",
      "Sex1                         3.527e-02  8.415e-02  1.599e+01   0.419  0.68072\n",
      "Post.onset.weeks             5.977e-03  8.333e-02  1.606e+01   0.072  0.94371\n",
      "logfreq                     -2.030e-02  1.899e-02  6.955e+01  -1.069  0.28888\n",
      "POper:distr_dissimilarity   -8.738e-03  1.962e-02  6.835e+02  -0.445  0.65614\n",
      "POper:consec_dissimilarity   5.400e-02  1.988e-02  7.065e+02   2.716  0.00677\n",
      "distr_dissimilarity:PTri    -1.858e-02  2.208e-02  6.871e+02  -0.841  0.40037\n",
      "consec_dissimilarity:PTri   -6.748e-02  2.298e-02  7.092e+02  -2.937  0.00342\n",
      "distr_dissimilarity:SLF_I    2.338e-03  2.896e-02  6.841e+02   0.081  0.93568\n",
      "consec_dissimilarity:SLF_I  -1.299e-02  2.992e-02  7.064e+02  -0.434  0.66426\n",
      "distr_dissimilarity:SLF_II   1.751e-02  2.884e-02  6.934e+02   0.607  0.54402\n",
      "consec_dissimilarity:SLF_II  3.170e-02  3.460e-02  7.083e+02   0.916  0.35989\n",
      "distr_dissimilarity:PreCG    5.536e-04  2.631e-02  6.889e+02   0.021  0.98322\n",
      "consec_dissimilarity:PreCG   3.118e-02  2.945e-02  7.146e+02   1.059  0.28999\n",
      "                               \n",
      "(Intercept)                 ***\n",
      "POper                          \n",
      "distr_dissimilarity            \n",
      "consec_dissimilarity           \n",
      "PTri                           \n",
      "SLF_I                          \n",
      "SLF_II                         \n",
      "PreCG                          \n",
      "Age                            \n",
      "Sex1                           \n",
      "Post.onset.weeks               \n",
      "logfreq                        \n",
      "POper:distr_dissimilarity      \n",
      "POper:consec_dissimilarity  ** \n",
      "distr_dissimilarity:PTri       \n",
      "consec_dissimilarity:PTri   ** \n",
      "distr_dissimilarity:SLF_I      \n",
      "consec_dissimilarity:SLF_I     \n",
      "distr_dissimilarity:SLF_II     \n",
      "consec_dissimilarity:SLF_II    \n",
      "distr_dissimilarity:PreCG      \n",
      "consec_dissimilarity:PreCG     \n",
      "---\n",
      "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cmd = 'C:\\\\\"Program Files\"\\\\R\\\\R-4.2.2\\\\bin\\\\Rscript utils\\\\neural_model_fitting.R'\n",
    "output = subprocess.check_output(cmd, universal_newlines=True, shell=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724a4cb0-a84c-4a35-929e-a3ebdd888b9c",
   "metadata": {},
   "source": [
    "Numerically assessing interference effects at the single-subject level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "765e7986-904d-4635-be34-b9dfa5baf118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N patients showing each interference effect:\n",
      " consec_eff    15\n",
      "lexint_eff    22\n",
      "semint_eff    12\n",
      "dtype: object\n",
      "Percent of patients showing each interference effect:\n",
      " consec_eff    60.0\n",
      "lexint_eff    88.0\n",
      "semint_eff    48.0\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# calculcating individual effect sizes\n",
    "from numpy import mean, std\n",
    "from math import sqrt\n",
    "def cohen_d(x,y):\n",
    "    return (mean(x) - mean(y)) / sqrt((std(x, ddof=1) ** 2 + std(y, ddof=1) ** 2) / 2.0)\n",
    "\n",
    "temp_id_vec = []\n",
    "temp_consec_effect_vec = []\n",
    "temp_lexint_effect_vec = []\n",
    "temp_semint_effect_vec = []\n",
    "\n",
    "for sub in pwi_data_clean['ID'].unique():\n",
    "\n",
    "    subset = pwi_data_clean[pwi_data_clean['ID'] == sub]\n",
    "    subset = subset.dropna(subset=['logRT', 'consec_dissimilarity'])\n",
    "    \n",
    "    temp_id_vec.append(sub)\n",
    "    \n",
    "    # calculating effect sizes\n",
    "    temp_model = smf.ols(formula='logRT ~ consec_dissimilarity', data=subset).fit()\n",
    "    temp_consec_effect_vec.append(temp_model.params['consec_dissimilarity'])\n",
    "    temp_semint_effect_vec.append(cohen_d(subset.loc[subset['Condition'] == 'unrel', 'logRT'],\n",
    "                                          subset.loc[subset['Condition'] == 'semantic', 'logRT']))\n",
    "    temp_lexint_effect_vec.append(cohen_d(subset.loc[subset['Condition'] == 'congr', 'logRT'],\n",
    "                                          subset.loc[subset['Condition'] == 'semantic', 'logRT']))\n",
    "\n",
    "\n",
    "eff_size = pd.DataFrame({'ID': temp_id_vec,\n",
    "                         'consec_eff': temp_consec_effect_vec,\n",
    "                         'lexint_eff': temp_lexint_effect_vec,\n",
    "                         'semint_eff': temp_semint_effect_vec})\n",
    "\n",
    "# calculating N subjects showing interference effects\n",
    "eff_size_binary = eff_size.copy()\n",
    "eff_size_binary[['consec_eff', 'lexint_eff', 'semint_eff']] = eff_size_binary[['consec_eff', 'lexint_eff', 'semint_eff']]<0\n",
    "print('N patients showing each interference effect:\\n', eff_size_binary.sum()[['consec_eff', 'lexint_eff', 'semint_eff']])\n",
    "print('Percent of patients showing each interference effect:\\n', eff_size_binary.sum()[['consec_eff', 'lexint_eff', 'semint_eff']]*100/25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310a26fe-e5a0-4744-b14c-1fd7ce31727c",
   "metadata": {},
   "source": [
    "Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce6e237f-0e6a-4856-bb63-f06c4c593607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\azyryanov\\AppData\\Local\\Temp\\ipykernel_12456\\1511090745.py:45: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  sf, audio = wavfile.read(op.join('data', 'example_trials_cprin325', str(r['Critical.segment'])+'.wav'))\n"
     ]
    }
   ],
   "source": [
    "# filtering trials to plot task design\n",
    "example_trials = pd.read_csv(op.join('data', 'example_trials_cprin325', 'example_trials.csv'))\n",
    "\n",
    "# 1A - TASK DESIGN\n",
    "cond_colors = {'Unrelated': '#de3d82', 'Congruent': '#9d57f4', 'Semantically\\\\nrelated': 'black'}\n",
    "\n",
    "fig, ax = plt.subplots(nrows=6, ncols = 7,\n",
    "                       figsize = (7.087, 3),\n",
    "                       gridspec_kw=dict(height_ratios=[1.5,1,1.75,0.5,1,1],\n",
    "                                        width_ratios=[1.5,1,1,1,1,1,1]))\n",
    "\n",
    "for ir in range(6):\n",
    "    for ic in range(7):\n",
    "        if (ir == 4 and ic == 1) or (ir == 5 and ic == 1):\n",
    "            ax[ir, ic].spines[['top', 'bottom', 'right']].set_visible(False)\n",
    "            ax[ir, ic].set_xticks([])\n",
    "        else:\n",
    "            ax[ir, ic].set_axis_off()\n",
    "\n",
    "ax[0,0].annotate('Stimulus features', (0, -0.4), xycoords='axes fraction',\n",
    "                 fontsize = 7, weight='bold', va='bottom', ha = 'center', rotation=90)\n",
    "\n",
    "ax[2,0].annotate('Response\\nannotations', (0, 0.5), xycoords='axes fraction',\n",
    "                 fontsize = 7, weight='bold', va='center', ha = 'center', rotation=90)\n",
    "\n",
    "ax[4,0].annotate('Response latency\\npredictors', (0, 0.5), xycoords='axes fraction',\n",
    "                 fontsize = 7, weight='bold', va='center', ha = 'center', rotation=90)\n",
    "\n",
    "# plotting row labels\n",
    "ax[0,0].annotate('Stimulus', (0.15, 0.5), xycoords='axes fraction', fontsize = 7, va='center')\n",
    "ax[1,0].annotate('Target', (0.15, 0.66), xycoords='axes fraction', fontsize = 7, va='center')\n",
    "ax[1,0].annotate('Distractor', (0.15, 0.33), xycoords='axes fraction', fontsize = 7, va='center')\n",
    "ax[2,0].annotate('Response lemma', (0.15, 0.7), xycoords='axes fraction', fontsize = 7, va='center')\n",
    "ax[2,0].annotate('Latency, s', (0.15, 0.43), xycoords='axes fraction', fontsize = 7, va='center')\n",
    "ax[2,0].annotate('Preceding response\\nlemma', (0.15, 0.05), xycoords='axes fraction', fontsize = 7, va='center')\n",
    "ax[3,0].annotate('Condition', (0.15, 0.5), xycoords='axes fraction', fontsize = 7, va='center')\n",
    "ax[4,0].annotate('Target-distractor\\ndissimilarity', (0.15, 0.5), xycoords='axes fraction', fontsize = 7, va='center')\n",
    "ax[5,0].annotate('Sequential response\\ndissimilarity', (0.15, 0.5), xycoords='axes fraction', fontsize = 7, va='center')\n",
    "\n",
    "# plotting response parameters\n",
    "for i, (ri, r) in enumerate(example_trials.iterrows()):\n",
    "    ax[1,i+1].annotate(r['target_eng'], (0, 0.66), xycoords='axes fraction', fontsize = 7, va='center')\n",
    "    ax[1,i+1].annotate(r['distractor_eng'], (0, 0.33), xycoords='axes fraction', fontsize = 7, va='center')\n",
    "    \n",
    "    sf, audio = wavfile.read(op.join('data', 'example_trials_cprin325', str(r['Critical.segment'])+'.wav'))\n",
    "    tvec = np.linspace(0, audio.shape[0]/sf, audio.shape[0])\n",
    "    ax[2,i+1].plot(tvec, (audio-audio.mean())/audio.max(), color = 'lightgrey', linewidth=0.5)\n",
    "    ax[2,i+1].set_xlim(0, 2.2)\n",
    "    ax[2,i+1].set_ylim(-1.2, 1)\n",
    "    ax[2,i+1].hlines(y=-0.15, xmin=0, xmax = r['RT']/1000-0.05, color = 'black', linewidth=2.5)\n",
    "    ax[2,i+1].annotate(round(r['RT']/1000, 3), (0, 0.3), xycoords='axes fraction', fontsize = 7, va='center')\n",
    "    ax[2,i+1].annotate(r['Lemma_eng'], (0, 0.7), xycoords='axes fraction', fontsize = 7, va='center')\n",
    "    ax[2,i+1].annotate(r['prev_lemma_eng'], (0, 0.05), xycoords='axes fraction', fontsize = 7, va='center')\n",
    "    \n",
    "    ax[3,i+1].annotate('\\n'.join(r['Condition'].split('\\\\n')), (0, 0.5),\n",
    "                       xycoords='axes fraction', fontsize = 7, va='center',\n",
    "                       color = cond_colors[r['Condition']])\n",
    "    if np.isnan(r['distr_dissimilarity']):\n",
    "        ax[4,i+1].annotate('Not\\ndefined', (0, 0.5), xycoords='axes fraction', fontsize = 7, va='center')\n",
    "    ax[4,i+1].vlines(x=0.05, ymin=0, ymax = r['distr_dissimilarity'], color = 'black', linewidth=2.5)\n",
    "    ax[4,i+1].set_ylim(0.3,1)\n",
    "    ax[4,i+1].set_xlim(0,1)\n",
    "    \n",
    "    ax[5,i+1].vlines(x=0.05, ymin=0, ymax = r['consec_dissimilarity'], color = '#008c87', linewidth=2.5)\n",
    "    ax[5,i+1].set_ylim(0.3,1)\n",
    "    ax[5,i+1].set_xlim(0,1)\n",
    "    \n",
    "ax[4,1].set_yticks([0.3,1])\n",
    "ax[5,1].set_yticks([0.3,1])\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(hspace=0.3, wspace=0.1)\n",
    "\n",
    "plt.savefig(op.join('output', 'figure1_top_v2.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "373b67f1-3e8c-46db-aaf6-b9fc0b801933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarizing RTs for plotting\n",
    "pwi_rt_summary = pwi_data_clean.copy().pivot_table(index = ['ID', 'Condition'],\n",
    "                                                   values = 'logRT',\n",
    "                                                   aggfunc = 'mean').reset_index()\n",
    "pwi_rt_summary['meanRT'] = np.exp(pwi_rt_summary['logRT'])/1000\n",
    "\n",
    "\n",
    "fig, ax = plt.subplot_mosaic([['semint_rt', 'lexint_rt', 'consec_rt', 'venn']],\n",
    "                             figsize = (7.087, 2.2),\n",
    "                             gridspec_kw=dict(width_ratios=[1.5,1.5,2.7,2]))\n",
    "\n",
    "# some common styling\n",
    "effect_color = {'semint_rt': '#de3d82', 'lexint_rt': '#9d57f4', 'consec_rt': '#008c87'}\n",
    "sns.set_style('ticks')\n",
    "\n",
    "def remove_half_violin(axis):\n",
    "    xlim = axis.get_xlim()\n",
    "    ylim = axis.get_ylim()\n",
    "    for violin in axis.collections:\n",
    "        bbox = violin.get_paths()[0].get_extents()\n",
    "        x0, y0, width, height = bbox.bounds\n",
    "        violin.set_clip_path(plt.Rectangle((x0-0.05, y0), (width / 2) + 0.05, height,\n",
    "                                           transform=axis.transData))\n",
    "    return axis\n",
    "\n",
    "for a in ax:\n",
    "    ax[a].spines[['top', 'right']].set_visible(False)\n",
    "\n",
    "ax['semint_rt'].annotate('B', (-0.4, 1), xycoords='axes fraction', fontsize = 10, weight='bold', va='top')\n",
    "ax['consec_rt'].annotate('C', (-0.25, 1), xycoords='axes fraction', fontsize = 10, weight='bold', va='top')\n",
    "ax['venn'].annotate('D', (-0.4, 1), xycoords='axes fraction', fontsize = 10, weight='bold', va='top')\n",
    "\n",
    "# PANEL C - DISTRACTOR INTERFERENCE EFFECTS\n",
    "sns.violinplot(data=pwi_rt_summary[pwi_rt_summary['Condition'] != 'congr'],\n",
    "               x='Condition', y='meanRT', hue=None,\n",
    "               split=False, ax = ax['semint_rt'], cut = 0,\n",
    "               dodge=False, inner=None, color='lightgrey',\n",
    "               label = '_nolegend_')\n",
    "\n",
    "sns.boxplot(data=pwi_rt_summary[pwi_rt_summary['Condition'] != 'congr'],\n",
    "            x='Condition', y='meanRT', saturation=1, showfliers=False,\n",
    "            width=0.2, boxprops={'zorder': 3, 'facecolor': 'none'}, ax=ax['semint_rt'])\n",
    "\n",
    "for i, violin in enumerate(ax['semint_rt'].collections):\n",
    "    bbox = violin.get_paths()[0].get_extents()\n",
    "    x0, y0, width, height = bbox.bounds\n",
    "    if i == 0:\n",
    "        violin.set_clip_path(plt.Rectangle((x0-0.05, y0), (width / 2) + 0.05, height,\n",
    "                                           transform=ax['semint_rt'].transData))\n",
    "    else:\n",
    "        violin.set_clip_path(plt.Rectangle((x0+width/2, y0), (width / 2) + 0.05, height,\n",
    "                                           transform=ax['semint_rt'].transData))\n",
    "        \n",
    "for i, sub in enumerate(pwi_rt_summary['ID'].unique()):\n",
    "    yvals = [pwi_rt_summary.loc[(pwi_rt_summary['Condition'] == c) & \\\n",
    "                                (pwi_rt_summary['ID'] == sub), 'meanRT'] for c in ['semantic', 'unrel']]\n",
    "    if yvals[0].values[0]>yvals[1].values[0]:\n",
    "        color = effect_color['semint_rt']\n",
    "    else:\n",
    "        color = 'black'\n",
    "    ax['semint_rt'].plot([0.25,0.75], yvals, marker = 'o', markersize = 1,\n",
    "                          linewidth = 0.5, color = color)\n",
    "\n",
    "ax['semint_rt'].set_ylabel('Mean response latency, s')\n",
    "ax['semint_rt'].set_yscale('log')\n",
    "ax['semint_rt'].set_ylim(0.7,4.5)\n",
    "ax['semint_rt'].set_yticks([0.7,1,2,3,4], labels=[0.7,1,2,3,4])\n",
    "ax['semint_rt'].set_xticks([0,1], labels = ['Semantic\\ncondition', 'Unrelated\\ncondition'])\n",
    "ax['semint_rt'].set_xlabel('')\n",
    "ax['semint_rt'].spines[['bottom']].set_visible(False)\n",
    "ax['semint_rt'].tick_params(axis='x', bottom=False)\n",
    "\n",
    "sns.violinplot(data=pwi_rt_summary[pwi_rt_summary['Condition'] != 'unrel'],\n",
    "               x='Condition', y='meanRT', hue=None,\n",
    "               split=False, ax = ax['lexint_rt'], cut = 0,\n",
    "               dodge=False, inner=None, color='lightgrey',\n",
    "               label = '_nolegend_')\n",
    "\n",
    "sns.boxplot(data=pwi_rt_summary[pwi_rt_summary['Condition'] != 'unrel'],\n",
    "            x='Condition', y='meanRT', saturation=1, showfliers=False,\n",
    "            width=0.2, boxprops={'zorder': 3, 'facecolor': 'none'}, ax=ax['lexint_rt'])\n",
    "\n",
    "for i, violin in enumerate(ax['lexint_rt'].collections):\n",
    "    bbox = violin.get_paths()[0].get_extents()\n",
    "    x0, y0, width, height = bbox.bounds\n",
    "    if i == 0:\n",
    "        violin.set_clip_path(plt.Rectangle((x0-0.05, y0), (width / 2) + 0.05, height,\n",
    "                                           transform=ax['lexint_rt'].transData))\n",
    "    else:\n",
    "        violin.set_clip_path(plt.Rectangle((x0+width/2, y0), (width / 2) + 0.05, height,\n",
    "                                           transform=ax['lexint_rt'].transData))\n",
    "        \n",
    "for i, sub in enumerate(pwi_rt_summary['ID'].unique()):\n",
    "    yvals = [pwi_rt_summary.loc[(pwi_rt_summary['Condition'] == c) & \\\n",
    "                                (pwi_rt_summary['ID'] == sub), 'meanRT'] for c in ['congr', 'semantic']]\n",
    "    \n",
    "    if yvals[0].values[0]<yvals[1].values[0]:\n",
    "        color = effect_color['lexint_rt']\n",
    "    else:\n",
    "        color = 'black'\n",
    "        \n",
    "    ax['lexint_rt'].plot([0.25,0.75], yvals, marker = 'o', markersize = 1,\n",
    "                          linewidth = 0.5, color = color)\n",
    "\n",
    "ax['lexint_rt'].set_ylabel(None)\n",
    "ax['lexint_rt'].set_yscale('log')\n",
    "ax['lexint_rt'].set_ylim(0.7,4.5)\n",
    "ax['lexint_rt'].set_yticks([0.7,1,2,3,4], labels=[])\n",
    "ax['lexint_rt'].set_xticks([0,1], labels = ['Congruent\\ncondition', 'Semantic\\ncondition'])\n",
    "ax['lexint_rt'].set_xlabel('')\n",
    "ax['lexint_rt'].spines[['bottom']].set_visible(False)\n",
    "ax['lexint_rt'].tick_params(axis='x', bottom=False)\n",
    "ax['lexint_rt'].set_xlim(ax['lexint_rt'].get_xlim()[::-1])\n",
    "\n",
    "# plotting sequential interference effects on RT\n",
    "for i, sub in enumerate(pwi_data_clean['ID'].unique()):\n",
    "    \n",
    "    subset_consec = pwi_data_clean.loc[(pwi_data_clean['ID'] == sub), ['logRT', 'consec_dissimilarity']]\n",
    "    subset_consec = subset_consec.dropna()\n",
    "   \n",
    "    # plotting\n",
    "    ax['consec_rt'].scatter(subset_consec['consec_dissimilarity'], subset_consec['logRT'],\n",
    "                            s=0.8, color = 'lightgrey', alpha = 0.5)\n",
    "    \n",
    "    temp_effsize = eff_size.loc[eff_size['ID'] == sub, 'consec_eff'].values[0]\n",
    "    \n",
    "    if temp_effsize<0:\n",
    "        color = effect_color['consec_rt']\n",
    "    else:\n",
    "        color = 'black'\n",
    "        \n",
    "    sns.regplot(data = subset_consec,\n",
    "                x = subset_consec['consec_dissimilarity'],\n",
    "                y = subset_consec['logRT'],\n",
    "                ax = ax['consec_rt'],\n",
    "                ci = None, color = color, marker = '',\n",
    "                line_kws={'linewidth':0.5, 'alpha':1})\n",
    "\n",
    "ax['consec_rt'].set_ylabel('Response latency, s')\n",
    "ax['consec_rt'].set_xlabel('Sequential response dissimilarity')\n",
    "ax['consec_rt'].spines[['top', 'right']].set_visible(False)\n",
    "ax['consec_rt'].set_yticks(np.log([500,1000,3000,10000]), labels=[0.5,1,3,10])\n",
    "\n",
    "# PANEL D -- VENN DIAGRAM\n",
    "ax['venn'].set_axis_off()\n",
    "lexint_ids = set(eff_size_binary.loc[eff_size_binary['lexint_eff'], 'ID'])\n",
    "semint_ids = set(eff_size_binary.loc[eff_size_binary['semint_eff'], 'ID'])\n",
    "consec_ids = set(eff_size_binary.loc[eff_size_binary['consec_eff'], 'ID'])\n",
    "\n",
    "v = venn3([lexint_ids, semint_ids, consec_ids], ('', '', ''), ax = ax['venn'])\n",
    "\n",
    "v.patches[0].set_color('#9d57f4')\n",
    "v.patches[2].set_color('#de3d82')\n",
    "v.patches[3].set_color('#008c87')\n",
    "v.patches[6].set_color('black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(hspace=0.3, wspace=0.5)\n",
    "plt.savefig(op.join('output', 'figure1_bottom_v2.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0427459e-dfb7-451e-beae-6ab56c98b8d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "The relation between interference effect sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e302c99b-0765-4429-a673-2143762c8c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:             lexint_eff   R-squared:                       0.090\n",
      "Model:                            OLS   Adj. R-squared:                  0.008\n",
      "Method:                 Least Squares   F-statistic:                     1.093\n",
      "Date:                Wed, 25 Sep 2024   Prob (F-statistic):              0.353\n",
      "Time:                        13:47:31   Log-Likelihood:                -25.591\n",
      "No. Observations:                  25   AIC:                             57.18\n",
      "Df Residuals:                      22   BIC:                             60.84\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -0.7068      0.146     -4.842      0.000      -1.010      -0.404\n",
      "consec_eff    -0.1876      0.333     -0.564      0.579      -0.878       0.502\n",
      "semint_eff     0.6808      0.481      1.416      0.171      -0.316       1.678\n",
      "==============================================================================\n",
      "Omnibus:                        4.742   Durbin-Watson:                   1.968\n",
      "Prob(Omnibus):                  0.093   Jarque-Bera (JB):                3.356\n",
      "Skew:                          -0.889   Prob(JB):                        0.187\n",
      "Kurtosis:                       3.249   Cond. No.                         3.37\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "effect_size_model = smf.ols(formula='lexint_eff ~ consec_eff + semint_eff',\n",
    "                            data=eff_size).fit()\n",
    "print(effect_size_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8dcc3c-ff16-4ff6-a92f-4b85b98c3800",
   "metadata": {},
   "source": [
    "Summarizing lesion volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b40703ab-fbf6-4c65-b2f4-973606ba453b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lesion_volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>65.475440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>60.088772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.532000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>27.858000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>44.077000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>85.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>213.998000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       lesion_volume\n",
       "count      25.000000\n",
       "mean       65.475440\n",
       "std        60.088772\n",
       "min         5.532000\n",
       "25%        27.858000\n",
       "50%        44.077000\n",
       "75%        85.740000\n",
       "max       213.998000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lesion volume summary\n",
    "# calculating normalized lesion volumes\n",
    "lesion_vols = cortical_loads.groupby('ID')[['numVoxNotZero']].sum()/1000\n",
    "lesion_vols = lesion_vols.rename({'numVoxNotZero': 'lesion_volume'}, axis=1)\n",
    "\n",
    "lesion_vols.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4254df1c-31ec-4b20-8c83-23d53f077952",
   "metadata": {},
   "source": [
    "Figure 2. Lesion characteristics\n",
    "\n",
    "* A -- lesion overlay (plotted separately with MRICroGL)\n",
    "* B -- lesion load distributions\n",
    "* C -- lesion load corrplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "060ec8bc-c8c7-4620-8948-78c892e940d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplot_mosaic([['overlay', 'load_distr', 'load_corrmat']],\n",
    "                             figsize = (7.087,2.5),\n",
    "                             gridspec_kw=dict(width_ratios=[1.5,1.3,1]))\n",
    "\n",
    "# plotting lesion load distributions\n",
    "inf_rois = ['PreCG', 'POper', 'PTri']\n",
    "inf_colors = ['#ffbb63', '#ffb2ce', '#dbbbfe']\n",
    "sup_rois = ['SLF_I', 'SLF_II', 'SLF_III']\n",
    "sup_colors = ['#65dad2', '#67dea8', '#bce92a']\n",
    "\n",
    "neural_predictors_long = pd.melt(neural_predictors,\n",
    "                                 id_vars='ID',\n",
    "                                 value_vars=inf_rois + sup_rois,\n",
    "                                 var_name='roi',\n",
    "                                 value_name='load')\n",
    "\n",
    "sns.violinplot(data=neural_predictors_long,\n",
    "               x='roi', y='load', hue='roi',\n",
    "               dodge=False, inner=None, color = 'grey', order = inf_rois + sup_rois,\n",
    "               palette = inf_colors + sup_colors,\n",
    "               ax = ax['load_distr'], cut=0, linewidth=0)\n",
    "\n",
    "for i, sub in enumerate(neural_predictors['ID'].unique()):\n",
    "\n",
    "    ax['load_distr'].plot(np.arange(len(inf_rois)),\n",
    "                          neural_predictors.loc[neural_predictors['ID'] == sub, inf_rois].values.squeeze(),\n",
    "                          marker = 'o', markersize = 1,\n",
    "                          linewidth = 0.3, color = 'black')\n",
    "    \n",
    "    ax['load_distr'].plot(np.arange(len(sup_rois)) + 3,\n",
    "                          neural_predictors.loc[neural_predictors['ID'] == sub, sup_rois].values.squeeze(),\n",
    "                          marker = 'o', markersize = 1,\n",
    "                          linewidth = 0.3, color = 'black')\n",
    "\n",
    "labels = [l.replace('_', ' ') for l in inf_rois + sup_rois]\n",
    "ax['load_distr'].set_xticks(np.arange(len(inf_rois + sup_rois)), labels = labels)\n",
    "ax['load_distr'].set_ylabel('Lesion load')\n",
    "ax['load_distr'].set_xlabel('Region of interest')\n",
    "\n",
    "# add inferior frontal and fronto-parietal labels\n",
    "ax['load_distr'].set_ylim(0.0,1)\n",
    "ax['load_distr'].set_yticks([0,0.5,1])\n",
    "ax['load_distr'].text(1,1,'Inferior frontal\\ncortex', ha='center', va='top')\n",
    "ax['load_distr'].text(4,1,'Fronto-parietal\\nconnectivity', ha='center', va='top')\n",
    "\n",
    "# remove spines\n",
    "ax['load_distr'].spines[['top', 'bottom', 'right']].set_visible(False)\n",
    "ax['load_distr'].tick_params(axis='x', bottom=False)\n",
    "\n",
    "corr = neural_predictors[inf_rois + sup_rois].corr()\n",
    "mask = np.zeros_like(corr, dtype=bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "np.fill_diagonal(mask, False)\n",
    "\n",
    "heatmap = sns.heatmap(corr,\n",
    "                      annot=True,\n",
    "                      fmt='.2f',\n",
    "                      linewidths=0.5,\n",
    "                      cmap='RdBu_r',\n",
    "                      mask=mask,\n",
    "                      ax=ax['load_corrmat'],\n",
    "                      square=True,\n",
    "                      cbar=False, vmin=-1,vmax=1)\n",
    "\n",
    "# add colorbar\n",
    "norm = plt.Normalize(-1,1)\n",
    "sm = plt.cm.ScalarMappable(cmap=\"RdBu_r\", norm=norm)\n",
    "\n",
    "cax = ax['load_corrmat'].inset_axes([0.6, 0.95, 0.35, 0.05], in_layout = False)\n",
    "cb = fig.colorbar(sm, cax=cax, orientation = 'horizontal')\n",
    "cax.set_xticks([-1,0,1])\n",
    "cax.set_xlabel('Pearson $r$')\n",
    "\n",
    "ax['load_corrmat'].tick_params(axis='both', size=0)\n",
    "\n",
    "ax['overlay'].set_xticks([])\n",
    "ax['overlay'].set_yticks([])\n",
    "\n",
    "norm = plt.Normalize(0,19)\n",
    "sm = plt.cm.ScalarMappable(cmap=\"plasma\", norm=norm)\n",
    "cax = ax['overlay'].inset_axes([0, 0.95, 0.3, 0.04], in_layout = False)\n",
    "cb = fig.colorbar(sm, cax=cax, orientation = 'horizontal')\n",
    "cax.set_xticks([0,10,19])\n",
    "cax.set_xlabel('N lesioned')\n",
    "\n",
    "# overlay code Z -62 -58 -50; -44 -38 -32; -26 -20 -14\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(op.join('output', 'figure2_v2.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f181beb-19a1-43f8-b9b9-dcb802dc6102",
   "metadata": {},
   "source": [
    "Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0fa8001-206e-4566-bb50-6813164709d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>congr</th>\n",
       "      <th>semantic</th>\n",
       "      <th>unrel</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>mid</th>\n",
       "      <th>POper</th>\n",
       "      <th>PTri</th>\n",
       "      <th>PreCG</th>\n",
       "      <th>SLF_I</th>\n",
       "      <th>SLF_II</th>\n",
       "      <th>SLF_III</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cprin229</th>\n",
       "      <td>7.239626</td>\n",
       "      <td>7.143659</td>\n",
       "      <td>7.189506</td>\n",
       "      <td>7.249993</td>\n",
       "      <td>7.191968</td>\n",
       "      <td>7.112184</td>\n",
       "      <td>0.114805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.124404</td>\n",
       "      <td>0.002511</td>\n",
       "      <td>0.148373</td>\n",
       "      <td>0.230125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cprin268</th>\n",
       "      <td>6.978686</td>\n",
       "      <td>7.187864</td>\n",
       "      <td>7.173845</td>\n",
       "      <td>7.280231</td>\n",
       "      <td>7.009238</td>\n",
       "      <td>7.212147</td>\n",
       "      <td>0.420899</td>\n",
       "      <td>0.027185</td>\n",
       "      <td>0.122674</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.094437</td>\n",
       "      <td>0.266915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cprin282</th>\n",
       "      <td>7.198122</td>\n",
       "      <td>7.688929</td>\n",
       "      <td>7.660220</td>\n",
       "      <td>7.744902</td>\n",
       "      <td>7.718803</td>\n",
       "      <td>7.609114</td>\n",
       "      <td>0.008798</td>\n",
       "      <td>0.010055</td>\n",
       "      <td>0.004497</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>0.049046</td>\n",
       "      <td>0.087303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cprin293</th>\n",
       "      <td>7.309598</td>\n",
       "      <td>7.657168</td>\n",
       "      <td>7.628401</td>\n",
       "      <td>7.537637</td>\n",
       "      <td>7.534975</td>\n",
       "      <td>7.758560</td>\n",
       "      <td>0.371566</td>\n",
       "      <td>0.331276</td>\n",
       "      <td>0.262587</td>\n",
       "      <td>0.243034</td>\n",
       "      <td>0.895784</td>\n",
       "      <td>0.795703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cprin323</th>\n",
       "      <td>7.326393</td>\n",
       "      <td>7.553640</td>\n",
       "      <td>7.604545</td>\n",
       "      <td>7.676932</td>\n",
       "      <td>7.216959</td>\n",
       "      <td>7.699300</td>\n",
       "      <td>0.356777</td>\n",
       "      <td>0.009789</td>\n",
       "      <td>0.103334</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090493</td>\n",
       "      <td>0.259061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             congr  semantic     unrel      high       low       mid  \\\n",
       "ID                                                                     \n",
       "cprin229  7.239626  7.143659  7.189506  7.249993  7.191968  7.112184   \n",
       "cprin268  6.978686  7.187864  7.173845  7.280231  7.009238  7.212147   \n",
       "cprin282  7.198122  7.688929  7.660220  7.744902  7.718803  7.609114   \n",
       "cprin293  7.309598  7.657168  7.628401  7.537637  7.534975  7.758560   \n",
       "cprin323  7.326393  7.553640  7.604545  7.676932  7.216959  7.699300   \n",
       "\n",
       "             POper      PTri     PreCG     SLF_I    SLF_II   SLF_III  \n",
       "ID                                                                    \n",
       "cprin229  0.114805  0.000000  0.124404  0.002511  0.148373  0.230125  \n",
       "cprin268  0.420899  0.027185  0.122674  0.000093  0.094437  0.266915  \n",
       "cprin282  0.008798  0.010055  0.004497  0.001285  0.049046  0.087303  \n",
       "cprin293  0.371566  0.331276  0.262587  0.243034  0.895784  0.795703  \n",
       "cprin323  0.356777  0.009789  0.103334  0.000000  0.090493  0.259061  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preparing condition averages for plotting\n",
    "\n",
    "# getting quantiles for sequential resp dissimilarity by subject\n",
    "def q25(vals):\n",
    "    return vals.quantile(.25)\n",
    "\n",
    "def q75(vals):\n",
    "    return vals.quantile(.75)\n",
    "\n",
    "consec_dissim_quantiles = pwi_data_clean.loc[pwi_data_clean['Condition'] != 'congr']\n",
    "consec_dissim_quantiles = consec_dissim_quantiles.pivot_table(index = 'ID',\n",
    "                                                              values = 'consec_dissimilarity',\n",
    "                                                              aggfunc = [q25, q75])\n",
    "consec_dissim_quantiles.columns = consec_dissim_quantiles.columns.to_series().str.join('_')\n",
    "\n",
    "# binning sequential resp dissimilarity in the orig DF\n",
    "pwi_data_clean = pwi_data_clean.merge(consec_dissim_quantiles, how = 'left', on ='ID')\n",
    "pwi_data_clean['consec_bin'] = 'mid'\n",
    "pwi_data_clean.loc[pwi_data_clean['consec_dissimilarity']<=pwi_data_clean['q25_consec_dissimilarity'],\n",
    "                   'consec_bin'] = 'low'\n",
    "pwi_data_clean.loc[pwi_data_clean['consec_dissimilarity']>=pwi_data_clean['q75_consec_dissimilarity'],\n",
    "                   'consec_bin'] = 'high'\n",
    "\n",
    "# averaging within condition/bin\n",
    "mean_distrcond = pwi_data_clean.pivot_table(index = ['ID', 'Condition'],\n",
    "                                            values = 'logRT',\n",
    "                                            aggfunc = 'mean').reset_index()\n",
    "mean_distrcond = mean_distrcond.pivot(index='ID', columns='Condition', values='logRT')\n",
    "\n",
    "mean_consecbin = pwi_data_clean[pwi_data_clean['Condition'] != 'congr'].pivot_table(index = ['ID', 'consec_bin'],\n",
    "                                                                                    values = 'logRT',\n",
    "                                                                                    aggfunc = 'mean').reset_index()\n",
    "mean_consecbin = mean_consecbin.pivot(index='ID', columns='consec_bin', values='logRT')\n",
    "\n",
    "means_toplot = mean_distrcond.merge(mean_consecbin, how = 'left', on = 'ID')\n",
    "means_toplot = means_toplot.merge(neural_predictors, how = 'left', on = 'ID').set_index('ID')\n",
    "means_toplot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f36d2fbe-0c35-4d6d-b0f6-75db2d5e4aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking interaction stats from R models \n",
    "sig_inter = {'POper_high_low': (0.05, 2.72, 0.01, True),\n",
    "             'POper_congr_semantic': (0.09, 2.45, 0.01 ,True),\n",
    "             'POper_unrel_semantic': (-0.02, -0.43, 0.67,False),\n",
    "             'PTri_high_low': (-0.07, -2.94, 0.003, True),\n",
    "             'PTri_congr_semantic': ('-0.20', -5.05, '< 0.001', True),\n",
    "             'PTri_unrel_semantic': (0.01, 0.25, '0.80', False),\n",
    "             'SLF_I_high_low': (-0.01, -0.43, 0.66, False),\n",
    "             'SLF_I_congr_semantic': (0.15, 2.82, 0.005, True),\n",
    "             'SLF_I_unrel_semantic': (-0.03, -0.57, 0.57, False)}\n",
    "\n",
    "fig, ax = plt.subplots(nrows = 3, ncols = 4,\n",
    "                       figsize = (7.087,4.75),\n",
    "                       gridspec_kw=dict(width_ratios=[0.9,1,1,0.2]),\n",
    "                       sharey=True, sharex=True)\n",
    "\n",
    "effect_color = {'unrel_semantic': '#de3d82', 'congr_semantic': '#9d57f4', 'high_low': '#008c87'}\n",
    "\n",
    "for r, roi in enumerate(['PTri', 'POper', 'SLF_I']):\n",
    "    for c, effect in enumerate(['unrel_semantic', 'congr_semantic', 'high_low']):\n",
    "        \n",
    "        color = effect_color[effect]\n",
    "        alpha = 1\n",
    "        \n",
    "        # plotting single-subject interference effects (vertical lines)\n",
    "        condpair = effect.split('_')\n",
    "        for s in pwi_data_clean['ID'].unique():\n",
    "            load = means_toplot.loc[s, roi]\n",
    "            yvals = means_toplot.loc[s, condpair].values\n",
    "            \n",
    "            ax[r,c].plot(np.zeros(2) + load,\n",
    "                         yvals,\n",
    "                         color = 'black',\n",
    "                         alpha = alpha,\n",
    "                         linewidth = 0.8)\n",
    "        \n",
    "        # plotting regression fits within condition (thick lines)\n",
    "        for pi, p in enumerate(condpair):\n",
    "            if pi == 1:\n",
    "                color = 'black'\n",
    "            ax[r,c].scatter(means_toplot[roi], means_toplot[p], color = color, s = 2)\n",
    "            sns.regplot(data=means_toplot, x=roi, y=p, ax=ax[r,c], ci=None,\n",
    "                        marker ='', line_kws={'linewidth':1.5, 'alpha':alpha}, color = color)\n",
    "        \n",
    "        ax[r,c].set_ylabel('')\n",
    "        ax[r,c].set_xlabel('')\n",
    "        \n",
    "        beta, tval, pval, sig = sig_inter[f'{roi}_{effect}']\n",
    "        if sig:\n",
    "            weight='bold'\n",
    "        else:\n",
    "            weight='regular'\n",
    "        ax[r,c].annotate(f'β = {beta}\\nt = {tval}\\np = {pval}',\n",
    "                         (0.7, 1), xycoords='axes fraction',\n",
    "                         fontsize = 7, ha = 'left', va='top', weight=weight)\n",
    "        \n",
    "# setting x and y ranges/ticks\n",
    "for r in range(3):\n",
    "    for c in range(3):\n",
    "        ax[r,c].set_xlim(-0.025, 0.5)\n",
    "        ax[r,c].set_xticks([0,0.25,0.5])\n",
    "        ax[r,c].spines[['top', 'right']].set_visible(False)\n",
    "\n",
    "for r in range(3):\n",
    "    ax[r,0].set_yticks(np.log([1000,2000,3000,4000]), labels = [1,2,3,4])\n",
    "\n",
    "ax[2,0].set_xlabel('Lesion load')\n",
    "ax[2,0].set_ylabel('Response latency, s')\n",
    "\n",
    "\n",
    "    \n",
    "for c, effect in enumerate(['Distractor interference:\\nsemantically related vs. unrelated',\n",
    "                            'Distractor interference:\\nsemantically related vs. congruent']): \n",
    "    temp_annot = ax[0,c].annotate(effect, (0, 1.05), xycoords='axes fraction', fontsize = 7,\n",
    "                                  ha = 'left')\n",
    "\n",
    "temp_annot = ax[0,2].annotate('Sequential response interference\\nlow vs. high sequential dissimilarity',\n",
    "                              (0, 1.05), xycoords='axes fraction',\n",
    "                              fontsize = 7, ha = 'left')\n",
    "\n",
    "# adding ROI and effect annotations\n",
    "for r, (roi, fullroi) in enumerate([('PTri', 'Pars\\ntriangularis'),\n",
    "                                    ('POper', 'Pars\\nopercularis'),\n",
    "                                    ('SLF_I', 'Superior longitudinal\\nfasciculus I')]): \n",
    "    temp_annot = ax[r,3].annotate(fullroi, (0.5, 0.2), xycoords='axes fraction',\n",
    "                                  va = 'center', ha = 'center', fontsize = 7, color = 'black')\n",
    "    ax[r,3].axis(\"off\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig(op.join('output', 'figure3_v3.pdf'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
